# K8s version

- query: kubernetes_build_info{cluster="{{.AKS_MC_CLUSTER_NAME}}"}
  metricName: k8sBuildInfo

- query: etcd_cluster_version{cluster="{{.AKS_MC_CLUSTER_NAME}}"}
  metricName: etcdVersion

# API server

- query: irate(apiserver_request_total{cluster="{{.AKS_MC_CLUSTER_NAME}}",verb=~"post|patch", resource="pods", code="200"}[2m]) > 0
  metricName: schedulingThroughput

- query: histogram_quantile(0.99, sum(irate(apiserver_request_duration_seconds_bucket{cluster="{{.AKS_MC_CLUSTER_NAME}}",verb=~"LIST|GET", subresource!~"log|exec|portforward|attach|proxy"}[2m])) by (le, resource, verb, scope)) > 0
  metricName: readOnlyAPICallsLatency

- query: histogram_quantile(0.99, sum(irate(apiserver_request_duration_seconds_bucket{cluster="{{.AKS_MC_CLUSTER_NAME}}",verb=~"POST|PUT|DELETE|PATCH", subresource!~"log|exec|portforward|attach|proxy"}[2m])) by (le, resource, verb, scope)) > 0
  metricName: mutatingAPICallsLatency

- query: sum(irate(apiserver_request_total{cluster="{{.AKS_MC_CLUSTER_NAME}}",verb!="WATCH"}[2m])) by (verb,resource,code) > 0
  metricName: APIRequestRate

# Process CPU and Memory metrics
- query: sum(irate(process_cpu_seconds_total{cluster=~".*{{.AKS_MC_CLUSTER_NAME}}",job="kube-apiserver"}[2m]) * 100) by (instance)
  metricName: APIServerCPU

- query: sum(process_resident_memory_bytes{cluster=~".*{{.AKS_MC_CLUSTER_NAME}}",job="kube-apiserver"}) by (instance)
  metricName: APIServerMemory
  
# ETCD latency & Etcd metrics

- query: sum(rate(etcd_server_leader_changes_seen_total{cluster="{{.AKS_MC_CLUSTER_NAME}}"}[2m]))
  metricName: etcdLeaderChangesRate

- query: histogram_quantile(0.99,rate(etcd_disk_backend_commit_duration_seconds_bucket{cluster="{{.AKS_MC_CLUSTER_NAME}}"}[2m]))
  metricName: 99thEtcdDiskBackendCommitDurationSeconds

- query: histogram_quantile(0.99,rate(etcd_disk_wal_fsync_duration_seconds_bucket{cluster="{{.AKS_MC_CLUSTER_NAME}}"}[2m]))
  metricName: 99thEtcdDiskWalFsyncDurationSeconds

- query: histogram_quantile(0.99,rate(etcd_network_peer_round_trip_time_seconds_bucket{cluster="{{.AKS_MC_CLUSTER_NAME}}"}[5m]))
  metricName: 99thEtcdRoundTripTimeSeconds
  
- query: histogram_quantile(0.99, sum(irate(etcd_request_duration_seconds_bucket{cluster="{{.AKS_MC_CLUSTER_NAME}}",type=~"configmaps|secrets|deamonsets.apps|deployments.apps|endpoints|events|pods",operation=~"LIST|GET"}[2m])) by (le, type, operation)) > 0
  metricName: readOnlyEtcdOperationLatency

- query: histogram_quantile(0.99, sum(irate(etcd_request_duration_seconds_bucket{cluster="{{.AKS_MC_CLUSTER_NAME}}",type=~"configmaps|secrets|deamonsets.apps|deployments.apps|endpoints|events|pods",operation=~"CREATE|DELETE|UPDATE"}[2m])) by (le, type, operation)) > 0
  metricName: writeEtcdOperationLatency


# Containers & pod metrics

- query: (sum(irate(container_cpu_usage_seconds_total{cluster="{{.AKS_MC_CLUSTER_NAME}}",name!=""}[2m]) * 100) by (container, pod, instance)) > 0
  metricName: containerCPU

- query: sum(container_memory_rss{cluster="{{.AKS_MC_CLUSTER_NAME}}",name!=""}) by (container, pod, instance)
  metricName: containerMemory

# Kubelet & Containerd runtime metrics

- query: sum(irate(process_cpu_seconds_total{cluster="{{.AKS_MC_CLUSTER_NAME}}",job="kubelet"}[2m]) * 100) by (instance) and on (instance) label_replace(kube_node_role{cluster="{{.AKS_MC_CLUSTER_NAME}}",role="worker"}, "instance", "$1", "node", "(.+)")
  metricName: kubeletCPU

- query: sum(process_resident_memory_bytes{cluster="{{.AKS_MC_CLUSTER_NAME}}",job="kubelet"}) by (instance) and on (instance) label_replace(kube_node_role{cluster="{{.AKS_MC_CLUSTER_NAME}}",role="worker"}, "instance", "$1", "node", "(.+)")
  metricName: kubeletMemory

# Containerd Usage

- query: (sum(irate(container_cpu_usage_seconds_total{cluster="{{.AKS_MC_CLUSTER_NAME}}",id="/system.slice/containerd.service"}[2m])*100) by (container, pod, instance)) > 0
  metricName: containerdCPU

- query: sum(container_memory_rss{cluster="{{.AKS_MC_CLUSTER_NAME}}",id="/system.slice/containerd.service"}) by (container, pod, instance)
  metricName: containerdMemory
  
# Node metrics: CPU & Memory

- query: (sum(irate(node_cpu_seconds_total[2m])) by (mode,instance) and on (instance) label_replace(kube_node_role{cluster="{{.AKS_MC_CLUSTER_NAME}}",role="worker"}, "instance", "$1", "node", "(.+)")) > 0
  metricName: nodeCPU-Workers

# We compute memory utilization by substrating available memory to the total

- query: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) and on (instance) label_replace(kube_node_role{cluster="{{.AKS_MC_CLUSTER_NAME}}",role="worker"}, "instance", "$1", "node", "(.+)")
  metricName: nodeMemoryUtilization-Workers

# Cluster metrics

- query: sum(kube_namespace_status_phase{cluster="{{.AKS_MC_CLUSTER_NAME}}"}) by (phase) > 0
  metricName: namespaceCount

- query: sum(kube_pod_status_phase{cluster="{{.AKS_MC_CLUSTER_NAME}}"}) by (phase)
  metricName: podStatusCount

- query: count(kube_secret_info{cluster="{{.AKS_MC_CLUSTER_NAME}}"})
  metricName: secretCount
  instant: true

- query: count(kube_deployment_labels{cluster="{{.AKS_MC_CLUSTER_NAME}}"})
  metricName: deploymentCount
  instant: true

- query: count(kube_configmap_info{cluster="{{.AKS_MC_CLUSTER_NAME}}"})
  metricName: configmapCount
  instant: true

- query: count(kube_service_info{cluster="{{.AKS_MC_CLUSTER_NAME}}"})
  metricName: serviceCount
  instant: true

- query: kube_node_role{cluster="{{.AKS_MC_CLUSTER_NAME}}"}
  metricName: nodeRoles

- query: sum(kube_node_status_condition{cluster="{{.AKS_MC_CLUSTER_NAME}}",status="true"}) by (condition)
  metricName: nodeStatus

- query: count(kube_replicaset_labels{cluster="{{.AKS_MC_CLUSTER_NAME}}"})
  metricName: replicaSetCount
  instant: true

- query: sum(kubelet_running_pods{cluster="{{.AKS_MC_CLUSTER_NAME}}"}) by (instance)
  metricName: podDistribution

# Kubeproxy and OVN service sync latency

- query: histogram_quantile(0.99, sum(rate(kubeproxy_network_programming_duration_seconds_bucket{cluster="{{.AKS_MC_CLUSTER_NAME}}"}[2m])) by (le)) > 0
  metricName: serviceSyncLatency

